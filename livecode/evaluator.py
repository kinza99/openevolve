import os
import sys
# å°†å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•æ·»åŠ åˆ° Python æ¨¡å—æœç´¢è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from lcb_runner.runner.main import MockArgs, generate
from litellm import completion
from typing import Any, Optional, Dict
import httpx
import openai
from lcb_runner.benchmarks import CodeGenerationProblem
from lcb_runner.runner.scenario_router import build_prompt_benchmark
from lcb_runner.evaluation.testing_util import run_test
from prompts.testing import system_prompt, assistant_prompt
import concurrent.futures
import time
import logging
import re
import json
from tqdm import tqdm
import datetime
import traceback

logger = logging.getLogger(__name__)

MODEL_NAME = "service_dv3_common"
API_KEY = "caa6246b-afbe-4d9b-ab34-87bf9922032b"
BASE_URL = "https://sd265fbi80c6ft26qc5ig.apigateway-cn-beijing.volceapi.com/v1/"

TEMPERATURE = 1.0
MAX_TOKENS = 32768

N = 16
NUM_WORKERS = 64

def create_openai_client(base_url: str, api_key: str):
    http_client = httpx.Client(
        limits=httpx.Limits(
            max_keepalive_connections=20,
            max_connections=40,
            keepalive_expiry=30.0
        ),
        timeout=httpx.Timeout(1800.0)
    )
    
    return openai.OpenAI(
        base_url=base_url,
        api_key=api_key,
        http_client=http_client
    )

client = create_openai_client(BASE_URL, API_KEY)

def make_config(**kwargs):
    return MockArgs(
        output_path='',
        model=MODEL_NAME,
        scenario="codegeneration",
        not_fast=False,
        n=N,
        release_version="release_latest",
        cot_code_execution=False,
        codegen_n=N,
        debug=False,
        continue_existing=False,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
        multiprocess=16,
        stop="###",
        continue_existing_with_eval=False,
        use_cache=False,
        cache_batch_size=100,
        evaluate=False,
        num_process_evaluate=1,
        timeout=60,
        openai_timeout=1800,
        start_date="2025-02-01",
        end_date="2025-03-01",
    )
    


def llm_call(prompt: list[dict[str, str]]):

    return client.chat.completions.create(
        model=MODEL_NAME,
        messages=prompt,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
    )

def extract_json_from_text_robust(text: str) -> tuple[Optional[Dict[str, Any]], Optional[str]]:
    """
    æ›´å¥å£®çš„ JSON æå–å‡½æ•°ï¼Œè¿”å›æœ€åä¸€ä¸ªæ‰¾åˆ°çš„ JSON å—å’Œé”™è¯¯ä¿¡æ¯
    
    Args:
        text: åŒ…å« JSON ä»£ç å—çš„æ–‡æœ¬
        
    Returns:
        tuple: (æœ€åä¸€ä¸ªæå–å‡ºçš„ JSON å¯¹è±¡æˆ– None, é”™è¯¯ä¿¡æ¯æˆ– None)
    """
    # æ”¯æŒå¤šç§ JSON ä»£ç å—æ ¼å¼
    patterns = [
        r'```json\s*\n(.*?)\n```',  # æ ‡å‡†æ ¼å¼
        r'```json(.*?)```',         # ç´§å‡‘æ ¼å¼ â† è¿™å°±æ˜¯æ‚¨é—®çš„è¿™ä¸ª
        r'`json\s*\n(.*?)\n`',      # å•åå¼•å·æ ¼å¼
        r'```\s*json\s*\n(.*?)\n```', # json å‰æœ‰ç©ºæ ¼
    ]
    
    all_errors = []
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
        
        if matches:
            # åªå¤„ç†æœ€åä¸€ä¸ªåŒ¹é…
            last_match = matches[-1]
            
            try:
                # æ¸…ç†å†…å®¹
                cleaned_match = last_match.strip()
                
                # å°è¯•è§£æ JSON
                json_obj = json.loads(cleaned_match)
                return json_obj, None
                
            except json.JSONDecodeError as e:
                all_errors.append(f"JSON parsing error: {str(e)}")
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤å¸¸è§é—®é¢˜
                try:
                    try:
                        # ç§»é™¤å¯èƒ½çš„æ³¨é‡Šå’Œå¤šä½™ç©ºç™½
                        cleaned = re.sub(r'//.*?\n', '', cleaned_match)
                        cleaned = re.sub(r'/\*.*?\*/', '', cleaned, flags=re.DOTALL)
                        json_obj = json.loads(cleaned.strip())
                        return json_obj, None
                    except Exception as e:
                        all_errors.append(f"JSON parsing error: {str(e)}")
                        try:
                            json_obj = eval(cleaned_match)
                            return json_obj, None
                        except Exception as e:
                            all_errors.append(f"Eval function parsing error: {str(e)}")
                            continue
                except json.JSONDecodeError as e:
                    all_errors.append(f"Final JSON parsing error: {str(e)}")
                    continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONå—
    if not any(re.search(pattern, text, re.DOTALL | re.IGNORECASE) for pattern in patterns):
        error_msg = "No JSON code blocks found in the response. Please ensure your response contains JSON wrapped in ```json ``` code blocks."
    else:
        error_msg = "JSON parsing failed with the following errors: " + all_errors[-1]
    
    return None, error_msg


def testing_generation_simple(problem: CodeGenerationProblem, idx, max_retry=10, delay=1):
    # if os.path.exists(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "testing.json")):
    #     with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "testing.json"), "r") as f:
    #         try:
    #             data = json.load(f)
    #             testing = data["testing_list"][idx]
    #             if testing is not None:
    #                 if (problem.starter_code == "") == (isinstance(testing[0]["input"], str)):
    #                     return data["output_list"][idx], testing
    #         except Exception as e:
    #             pass

    question_content = problem.question_content
    if problem.starter_code != "":
        tmp = f"""
Starter Code:
```python
{problem.starter_code}
```
"""
        question_content = question_content +'\n' + tmp
    
    prompt = [
        {
            "role": "system",
            "content": system_prompt
        },
        {
            "role": "user",
            "content": assistant_prompt.replace("{problem}", question_content)
        }
    ]
    content = None
    testing = None
    for attempt in range(max_retry):
        try:
            res = llm_call(prompt)
            if res.choices[0].finish_reason == "length":
                content = res.choices[0].message.content
                detailed_error_msg = "The input of unit test is too long which can be load by json.loads(), please use a simple python code to generate the input, the python code must can be process by the eval() function in python."
                logger.warning(f"testing_generation_simple ç¬¬{attempt+1}æ¬¡è°ƒç”¨å¤±è´¥: {detailed_error_msg}")
                # å°†LLMçš„å›å¤å’Œè¯¦ç»†é”™è¯¯æç¤ºæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
                prompt.append({
                    "role": "assistant", 
                    "content": content
                })
                prompt.append({
                    "role": "user",
                    "content": detailed_error_msg
                })
            else:
                content = res.choices[0].message.content
                testing, error_msg = extract_json_from_text_robust(content)
                
                if testing is not None:
                    if (problem.starter_code == "") != (isinstance(testing[0]["input"], str)):
                        if problem.starter_code == "":
                            error_msg = "Problem doesn't have starter code, the type of unit test input and output should be a string like Problem 2."
                        else:
                            error_msg = "Problem has starter code, the type of unit test input should be a function parameter dict like Problem 1."
                        logger.warning(f"testing_generation_simple ç¬¬{attempt+1}æ¬¡è°ƒç”¨å¤±è´¥: {error_msg}")
                        prompt.append({
                            "role": "assistant", 
                            "content": content
                        })
                        prompt.append({
                            "role": "user",
                            "content": error_msg
                        })
                    else:
                        break
                else:
                    # ä½¿ç”¨çœŸæ­£çš„é”™è¯¯ä¿¡æ¯ä½œä¸ºæ–°çš„å¯¹è¯å†…å®¹
                    detailed_error_msg = f"There was an issue with your JSON format. Error details: {error_msg}\n\nPlease fix the JSON format issues and regenerate the unit tests according to the specified format."
                    
                    logger.warning(f"testing_generation_simple ç¬¬{attempt+1}æ¬¡è°ƒç”¨å¤±è´¥: {error_msg}")
                    
                    # å°†LLMçš„å›å¤å’Œè¯¦ç»†é”™è¯¯æç¤ºæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
                    prompt.append({
                        "role": "assistant", 
                        "content": content
                    })
                    prompt.append({
                        "role": "user",
                        "content": detailed_error_msg
                    })
                
        except Exception as e:
            logger.warning(f"testing_generation_simple ç¬¬{attempt+1}æ¬¡è°ƒç”¨å¤±è´¥: {e}")
            if attempt < max_retry - 1:
                time.sleep(delay)
            else:
                logger.error(f'Testing Generation Error: {e}')
                raise
                
        if attempt < max_retry - 1:
            time.sleep(delay)
            
    return content, testing

def testing_generation_problem(problem: CodeGenerationProblem):
    # if os.path.exists(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "testing.json")):
    #     with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "testing.json"), "r") as f:
    #         data = json.load(f)
    #         return data
    
    results = []
    
    # æäº¤æ‰€æœ‰å­ä»»åŠ¡
    with concurrent.futures.ThreadPoolExecutor(max_workers=N) as executor:
        futures = [executor.submit(testing_generation_simple, problem, idx) for idx in range(N)]
        results = [future.result() for future in futures]
    
    good_result = []
    bad_result = []
    for result in results:
        if result[1] is not None:
            good_result.append(result)
        else:
            bad_result.append(result)
    
    testing_result = {
        "question_title": problem.question_title,
        "question_content": problem.question_content,
        "output_list": [result[0] for result in good_result+bad_result],
        "testing_list": [result[1] for result in good_result+bad_result]
    }
    os.makedirs(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}"), exist_ok=True)
    with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "testing.json"), "w") as f:
        json.dump(testing_result, f, indent=4)
    return testing_result
        
def testing_generation(dataset: list[CodeGenerationProblem]):
    """ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå¸¦è¿›åº¦æ¡"""
    print("ğŸ§ª å¼€å§‹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(testing_generation_problem, problem): problem 
                  for problem in dataset}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        results = []
        with tqdm(total=len(futures), desc="ğŸ“ ç”Ÿæˆæµ‹è¯•", 
                 bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',
                 colour='green') as pbar:
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    pbar.set_postfix({"å®Œæˆ": f"{len(results)}/{len(dataset)}"})
                    pbar.update(1)
                except Exception as e:
                    problem = futures[future]
                    logger.error(f"æµ‹è¯•ç”Ÿæˆå¤±è´¥ {problem.question_title}: {e}")
                    pbar.update(1)
    
    print(f"âœ… æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆ: {len(results)}/{len(dataset)}")
    return results

def solution_generation_problem(problem: CodeGenerationProblem):
    mock_args = make_config()
    os.makedirs(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}"), exist_ok=True)
    mock_args.output_path = os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "solution.json")
    generate(mock_args, problem)

def solution_generation(dataset: list[CodeGenerationProblem]):
    """ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œå¸¦è¿›åº¦æ¡"""
    print("ğŸ’¡ å¼€å§‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆ...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(solution_generation_problem, problem): problem 
                  for problem in dataset}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        results = []
        with tqdm(total=len(futures), desc="ğŸ”§ ç”Ÿæˆæ–¹æ¡ˆ", 
                 bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',
                 colour='blue') as pbar:
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    pbar.set_postfix({"å®Œæˆ": f"{len(results)}/{len(dataset)}"})
                    pbar.update(1)
                except Exception as e:
                    problem = futures[future]
                    logger.error(f"æ–¹æ¡ˆç”Ÿæˆå¤±è´¥ {problem.question_title}: {e}")
                    pbar.update(1)
    
    print(f"âœ… è§£å†³æ–¹æ¡ˆç”Ÿæˆå®Œæˆ: {len(results)} ä¸ªé—®é¢˜")
    return results

def verify(problem, solution_id, testing_id):
    solutions = []
    testings = []
    try:
        with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "solution.json"), "r") as f:
            data = json.load(f)
            solutions = data["code_list"]
        if testing_id != "ground_truth":
            with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "testing.json"), "r") as f:
                data = json.load(f)
                testings = data["testing_list"]
                testing = testings[int(testing_id)]
                inputs = []
                outputs = []  # ä¿®å¤ç¼©è¿›é—®é¢˜
            if testing is None:
                return {
                    "question_title": problem.question_title,
                    "solution_id": solution_id,
                    "testing_id": testing_id,
                    "result": False,
                    "unknown_error": True,
                    "metadata": {}
                }
            for test in testing:
                inputs.append(test["input"])
                outputs.append(test["output"])
            
            in_out = {"inputs": inputs, "outputs": outputs, "fn_name": problem.metadata.get('func_name', None)}
            sample = {"input_output": json.dumps(in_out)}
        else:
            sample = problem.get_evaluation_sample()

        solution = solutions[int(solution_id)]
    except Exception as e:
        logger.error(f"Verify Error: {e}")
        logger.error(traceback.format_exc())
        return {
                "question_title": problem.question_title,
                "solution_id": solution_id,
                "testing_id": testing_id,
                "result": False,
                "unknown_error": True,
                "metadata": {}
            }
    try:
        res, metadata = run_test(sample, solution, debug=False, timeout=60)
        result = True if metadata.get("execution time", None) != None else False
        unknown_error = False
    except Exception as e:
        logger.error(f"Verify Error: {e}")
        logger.error(traceback.format_exc())
        result = False
        unknown_error = True
        metadata = {}
    
    output = {
        "question_title": problem.question_title,
        "solution_id": solution_id,
        "testing_id": testing_id,
        "result": result,
        "unknown_error": unknown_error,
        "metadata": metadata
    }

    return output
    
def final_check_problem(problem: CodeGenerationProblem, sort_solution_id_list: list[int], sort_testing_id_list: list[int], top_s = 8):
    correct_solution_id_list = sort_solution_id_list[:top_s]
    wrong_solution_id_list = sort_solution_id_list[-top_s:]
    # correct_solution_id = sort_solution_id_list[0]
    # wrong_solution_id = sort_solution_id_list[-1]
    correct_testing_id = sort_testing_id_list[0]
    
    correct_output_list = []
    wrong_output_list = []
    for correct_solution_id in correct_solution_id_list:
        correct_output = verify(problem, correct_solution_id, correct_testing_id)
        correct_output_list.append(correct_output)
    for wrong_solution_id in wrong_solution_id_list:
        wrong_output = verify(problem, wrong_solution_id, correct_testing_id)
        wrong_output_list.append(wrong_output)
        
    ground_correct_output_list = []
    ground_wrong_output_list = []
    for correct_solution_id in correct_solution_id_list:
        ground_correct_output = verify(problem, correct_solution_id, "ground_truth")
        ground_correct_output_list.append(ground_correct_output)
    for wrong_solution_id in wrong_solution_id_list:
        ground_wrong_output = verify(problem, wrong_solution_id, "ground_truth")
        ground_wrong_output_list.append(ground_wrong_output)
        
    correct_result_list = [output['result'] for output in correct_output_list]
    wrong_result_list = [output['result'] for output in wrong_output_list]
    ground_correct_result_list = [output['result'] for output in ground_correct_output_list]
    ground_wrong_result_list = [output['result'] for output in ground_wrong_output_list]
    
    # correct_output = verify(problem, correct_solution_id, correct_testing_id)
    # wrong_output = verify(problem, wrong_solution_id, correct_testing_id)
    
    # ground_correct_output = verify(problem, correct_solution_id, "ground_truth")
    # ground_wrong_output = verify(problem, wrong_solution_id, "ground_truth")
    
    # correct_result = correct_output['result']
    # wrong_result = wrong_output['result']
    # ground_correct_result = ground_correct_output['result']
    # ground_wrong_result = ground_wrong_output['result']
    
    # unknown_error = correct_output_list[0]['unknown_error'] or wrong_output_list[-1]['unknown_error'] or ground_correct_output_list[0]['unknown_error'] or ground_wrong_output_list[-1]['unknown_error']
    unknown_error = False
    output = {
        "question_title": problem.question_title,
        "correct_solution_id": correct_solution_id,
        "correct_testing_id": correct_testing_id,
        "wrong_solution_id": wrong_solution_id,
        "correct_result": correct_result_list,
        "wrong_result": wrong_result_list,
        "ground_correct_result": ground_correct_result_list[0],
        "ground_wrong_result": ground_wrong_result_list[-1],
        "unknown_error": unknown_error,
        # "result": ground_correct_result and correct_result and (ground_wrong_result == wrong_result)
        "result": ground_correct_result_list[0] and (ground_correct_result_list == correct_result_list) and (ground_wrong_result_list == wrong_result_list)
    }
    os.makedirs(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}"), exist_ok=True)
    with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "final_check.json"), "w") as f:
        json.dump(output, f, indent=4)
    return output

def final_check(tasks):
    """æœ€ç»ˆæ£€æŸ¥ï¼Œå¸¦è¿›åº¦æ¡"""
    print("ğŸ” å¼€å§‹æœ€ç»ˆæ£€æŸ¥...")
    
    with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(final_check_problem, task[0], task[1], task[2]): task 
                  for task in tasks}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        results = []
        with tqdm(total=len(futures), desc="âœ… æœ€ç»ˆæ£€æŸ¥", 
                 bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',
                 colour='yellow') as pbar:
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    # å®æ—¶æ˜¾ç¤ºé€šè¿‡ç‡
                    passed = sum(1 for r in results if r['ground_correct_result'])
                    pass_rate = passed / len(results) * 100
                    pbar.set_postfix({
                        "å®Œæˆ": f"{len(results)}/{len(tasks)}",
                        "é€šè¿‡ç‡": f"{pass_rate:.1f}%"
                    })
                    pbar.update(1)
                except Exception as e:
                    task = futures[future]
                    logger.error(f"æœ€ç»ˆæ£€æŸ¥å¤±è´¥ {task[0].question_title}: {e}")
                    pbar.update(1)
    score = []
    answer_score = []
    for result in results:
        if not result['unknown_error']:
            if result['ground_correct_result']:
                answer_score.append(1)
            else:
                answer_score.append(0)
            if result['result']:
                score.append(1)
            else:
                score.append(0)
    final_score = sum(score) / len(score)
    final_answer_score = sum(answer_score) / len(answer_score)
    
    print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_score:.3f} ({score}/{len(score)})")
    print(f"ğŸ¯ ç­”æ¡ˆè¯„åˆ†: {final_answer_score:.3f} ({answer_score}/{len(answer_score)})")
    return final_score, final_answer_score

def generation_final_check_problem(problem: CodeGenerationProblem, sort_solution_id_list: list[int], sort_testing_id_list: list[int]):
    correct_solution_id = sort_solution_id_list[0]
    correct_testing_id = sort_testing_id_list[0]
    
    correct_output = verify(problem, correct_solution_id, correct_testing_id)
    
    correct_result = correct_output['result']
    
    unknown_error = correct_output['unknown_error']
    output = {
        "question_title": problem.question_title,
        "correct_solution_id": correct_solution_id,
        "correct_testing_id": correct_testing_id,
        "correct_result": correct_result,
        "unknown_error": unknown_error,
        "result": correct_result
    }
    os.makedirs(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}"), exist_ok=True)
    with open(os.path.join(problem.metadata['file_store_path'], f"{problem.question_title}", "generation_final_check.json"), "w") as f:
        json.dump(output, f, indent=4)
    return output

def generation_final_check(tasks: list[tuple[CodeGenerationProblem, list[int], list[int]]]):
    with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(generation_final_check_problem, task[0], task[1], task[2]): task 
                  for task in tasks}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        results = []
        with tqdm(total=len(futures), desc="âœ… ç”Ÿæˆæœ€ç»ˆæ£€æŸ¥", 
                 bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',
                 colour='yellow') as pbar:
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    # å®æ—¶æ˜¾ç¤ºé€šè¿‡ç‡
                    passed = sum(1 for r in results if r['result'])
                    pass_rate = passed / len(results) * 100
                    pbar.set_postfix({
                        "å®Œæˆ": f"{len(results)}/{len(tasks)}",
                        "é€šè¿‡ç‡": f"{pass_rate:.1f}%"
                    })
                    pbar.update(1)
                except Exception as e:
                    task = futures[future]
                    logger.error(f"æœ€ç»ˆæ£€æŸ¥å¤±è´¥ {task[0].question_title}: {e}")
                    pbar.update(1)
    score = []
    for result in results:
        if not result['unknown_error']:
            if result['result']:
                score.append(1)
            else:
                score.append(0)
    final_score = sum(score) / len(score)
    
    print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_score:.3f} ({score}/{len(score)})")
    return final_score

def strategy_wrapper(program_path, instance, solution_id_list, testing_id_list):
    """åŒ…è£…å‡½æ•°ï¼Œç”¨äºåœ¨å­è¿›ç¨‹ä¸­åŠ¨æ€å¯¼å…¥å¹¶æ‰§è¡Œstrategyå‡½æ•°"""
    import importlib.util
    
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°å¯¼å…¥æ¨¡å—
    spec = importlib.util.spec_from_file_location("program", program_path)
    program = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(program)
    
    # åœ¨å­è¿›ç¨‹ä¸­ä¹Ÿéœ€è¦é‡æ–°å®šä¹‰verifyå‡½æ•°ï¼Œå› ä¸ºå®ƒä¹Ÿä¸èƒ½è¢«pickle
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨strategyå‡½æ•°ï¼Œè®©å®ƒå¤„ç†verifyçš„é€»è¾‘
    return program.strategy(instance, solution_id_list, testing_id_list, verify)

def evaluate(program_path: str):
    """ä¸»è¯„ä¼°å‡½æ•°ï¼Œå¸¦å®Œæ•´è¿›åº¦æ˜¾ç¤º"""
    print("ğŸš€ å¼€å§‹è¯„ä¼°æµç¨‹...")
    
    current_time = datetime.datetime.now()
    FILE_STORE = "openevolve/livecode_file_store/init"
    
    print(f"ğŸ“ æ–‡ä»¶å­˜å‚¨è·¯å¾„: {FILE_STORE}")
    
    # æ„å»ºåŸºå‡†æµ‹è¯•
    print("ğŸ“‹ æ„å»ºåŸºå‡†æµ‹è¯•...")
    args = make_config()
    benchmark, _ = build_prompt_benchmark(args)
    for instance in benchmark:
        instance.metadata['file_store_path'] = FILE_STORE
    
    print(f"ğŸ“Š æ€»å…± {len(benchmark)} ä¸ªæµ‹è¯•é—®é¢˜")
    
    # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
    # testing_generation(benchmark)
    
    # ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
    # solution_generation(benchmark)
    
    # éªŒè¯é˜¶æ®µ
    print("ğŸ”„ å¼€å§‹éªŒè¯é˜¶æ®µ...")
    final_check_datasets = []
    
    with tqdm(benchmark, desc='ğŸ” ç­–ç•¥éªŒè¯', 
             bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',
             colour='cyan') as pbar:
        for instance in pbar:
            if os.path.exists(os.path.join(FILE_STORE, f"{instance.question_title}", "sort_list.json")):
                with open(os.path.join(FILE_STORE, f"{instance.question_title}", "sort_list.json"), "r") as f:
                    data = json.load(f)
                    sort_solution_id_list = data["sort_solution_id_list"]
                    sort_testing_id_list = data["sort_testing_id_list"]
            else:
                solution_id_list = testing_id_list = [str(i) for i in range(N)]
                sort_solution_id_list, sort_testing_id_list = strategy_wrapper(
                    program_path, instance, solution_id_list, testing_id_list
                )
                with open(os.path.join(FILE_STORE, f"{instance.question_title}", "sort_list.json"), "w") as f:
                    json.dump({"sort_solution_id_list": sort_solution_id_list, "sort_testing_id_list": sort_testing_id_list}, f, indent=4)
            final_check_datasets.append((instance, sort_solution_id_list, sort_testing_id_list))
            pbar.set_postfix({"é—®é¢˜": instance.question_title[:20] + "..."})
    
    # æœ€ç»ˆæ£€æŸ¥
    final_score, answer_score = final_check(final_check_datasets)
    
    print("ğŸ‰ è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ç»ˆå¾—åˆ†: {final_score:.3f}")
    print(f"ğŸ¯ ç­”æ¡ˆè¯„åˆ†: {answer_score:.3f}")
    
    return {"score": final_score}

def generation(dataset_path : str, strategy_path : str):
    current_time = datetime.datetime.now()
    FILE_STORE_PATH = 'livecode_data/file_store' + f'/{current_time.strftime("%Y%m%d%H%M%S")}'
    # args = make_config()
    new_datasets = []
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            instance = json.loads(line)
            livecode_instance = CodeGenerationProblem(
                question_title=instance['question_title'],
                question_content=instance['question_content'],
                starter_code=instance.get('starter_code', ''),
                metadata=json.dumps(instance['metadata']),
                public_test_cases='[]',
                private_test_cases='[]',
                difficulty=instance['difficulty'],
                platform=instance.get('platform', 'leetcode'),
                contest_id=instance.get('contest_id', 'fake_contest_id'),
                contest_date=instance.get('contest_date', '2030-01-01'),
                question_id=instance.get('idx', '')
            )
            livecode_instance.metadata['file_store_path'] = FILE_STORE_PATH
            new_datasets.append(livecode_instance)
    benchmark = new_datasets
    print(f"ğŸ“Š æ€»å…± {len(benchmark)} ä¸ªæµ‹è¯•é—®é¢˜")
    
    # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
    testing_generation(benchmark)
    
    # ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
    solution_generation(benchmark)
    
     # éªŒè¯é˜¶æ®µ
    print("ğŸ”„ å¼€å§‹éªŒè¯é˜¶æ®µ...")
    final_check_datasets = []
    
    with tqdm(benchmark, desc='ğŸ” ç­–ç•¥éªŒè¯', 
             bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',
             colour='cyan') as pbar:
        for instance in pbar:
            if os.path.exists(os.path.join(FILE_STORE_PATH, f"{instance.question_title}", "sort_list.json")):
                with open(os.path.join(FILE_STORE_PATH, f"{instance.question_title}", "sort_list.json"), "r") as f:
                    data = json.load(f)
                    sort_solution_id_list = data["sort_solution_id_list"]
                    sort_testing_id_list = data["sort_testing_id_list"]
            else:
                solution_id_list = testing_id_list = [str(i) for i in range(N)]
                sort_solution_id_list, sort_testing_id_list = strategy_wrapper(
                    strategy_path, instance, solution_id_list, testing_id_list
                )
            with open(os.path.join(FILE_STORE_PATH, f"{instance.question_title}", "sort_list.json"), "w") as f:
                json.dump({"sort_solution_id_list": sort_solution_id_list, "sort_testing_id_list": sort_testing_id_list}, f, indent=4)
            final_check_datasets.append((instance, sort_solution_id_list, sort_testing_id_list))
            pbar.set_postfix({"é—®é¢˜": instance.question_title[:20] + "..."})
    
    final_score = generation_final_check(final_check_datasets)
    
    print("ğŸ‰ è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ç»ˆå¾—åˆ†: {final_score:.3f}")
    
    return {"score": final_score}
    
if __name__ == "__main__":
    evaluate("livecode/init_program.py")
